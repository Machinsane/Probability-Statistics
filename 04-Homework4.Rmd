# Homework 4

```{r echo=FALSE, message=FALSE}
library(tidyverse)
library(knitr)
library(pander)
library(mosaic)
```

## Problem 1 NBC Pilot Survey

```{r echo=FALSE}
nbc = read.csv('/Users/kevin/Academic/MAE/2023 Summer/R&Prob&Stat/Assignments/Data/nbc_pilotsurvey.csv',header = TRUE)
```

### Part A

Question: Does either "Living with Ed" or "My Name is Earl" make audiences happier than the other one?

Approach: I will use t-test to check if the mean of "happy" scores for "Living with Ed" and "My Name is Earl" have any difference. Thus, in this case, the null hypothesis is the mean difference to be 0, which indicates that  they do not have any difference in terms of their means of happy "score".

Results:

```{r echo=FALSE}
ed = nbc %>% filter(Show=="Living with Ed") %>% select(Show,Q1_Happy) 
earl = nbc %>% filter(Show=="My Name is Earl") %>% select(Show,Q1_Happy)
t.test(ed$Q1_Happy, earl$Q1_Happy)
```

Conclusion: From the t test summary above, we can see 0 is in the 95% confidence interval. Thus, we fail to reject the null hypothesis here. Therefore, I conclude that there is not a significant evidence showing either "Living with Ed" or "My Name is Earl" is more funny than the other one.

### Part B

Question: Is either "The Biggest Loser" or "The Apprentice: Los Angeles" more annoying than the other one?

Approach: Likewise, I am going to use t-test to check if they have any difference in terms of the mean of "annoyed" scores. Therefore, the null hypothesis is the mean difference to be 0, which suggests that they have no difference on the mean of "annoyed" scores.

Results:

```{r echo=FALSE}
loser = nbc %>% filter(Show=="The Biggest Loser") %>% select(Show,Q1_Annoyed) 
la = nbc %>% filter(Show=="The Apprentice: Los Angeles") %>% select(Show,Q1_Annoyed)
t.test(loser$Q1_Annoyed, la$Q1_Annoyed)
```

Conclusion: From the t test summary above, we can see the 95% confident interval doesn't contain 0, which means our null hypothesis has to be rejected, so they really have difference in annoying level. To be more specific, The "Apprentice: Los Angeles" is more annoying than "The Biggest Loser", because it has higher average "annoyed" score.

### Part C

Question: As we expect, what proportion of American TV audiences would think "Dancing with the Stars" is confusing?

Approach: I am going to use proportion test with null hypothesis as P(not confusing) = P(confusing).

Results:

```{r echo=FALSE}
stars = nbc %>% filter(Show=="Dancing with the Stars") %>% select(Show,Q2_Confusing) %>% transmute(confusing=Q2_Confusing >= 4)
prop.test(stars$confusing)
```

Conclusion: From the proportions test summary above, we can see the null hypothesis fails to hold. Also, the 95% confident interval of actual proportion is from around 4.5% to 12.9%. In other words, The proportion of American audiences we expect to be befuddled by "Dancing with the Stars" falls into range between 4.5% to 12.9% with 95% confidence level.

## Problem 2 Ebay
```{r echo=FALSE}
ebay = read.csv('/Users/kevin/Academic/MAE/2023 Summer/R&Prob&Stat/Assignments/Data/ebay.csv',header = TRUE)
```

Question: Does the difference of revenue ratios between treatment group and control group indicate the paid search advertising on Google generates more revenue for EBay?

Approach: I am going to bootstrap 10000 times of mean of revenue ratios for both treatment group and control group, and then compute their differences. Once I have 10000 times of differences, I can create a 95% confidence interval of all differences to see whether 0 falls in to it or not. If 0 is in the interval, I can conclude that no significant evidence shows that paid search advertising boosts the revenue for EBay. If the interval falls into positive side, than I can say that paid search advertising diminish the revenue. If the interval falls into negative side, than I can say that paid search advertising boosts the revenue.

Results:

Let us see the histogram of the 10000 times bootstrap of the revenue ratio difference between treatmentgroup and control group:

```{r echo=FALSE}
treatment = ebay %>% filter(adwords_pause==1) %>% select(DMA,rev_ratio) 
ctrl = ebay %>% filter(adwords_pause==0) %>% select(DMA,rev_ratio)
```

```{r echo=FALSE}
boot_eaby = do(10000)*(mean(~rev_ratio, data=resample(treatment))-mean(~rev_ratio, data=resample(ctrl)))
```

```{r echo=FALSE}
ggplot(boot_eaby,aes(x=result)) + geom_histogram(bins=20,color="black", fill="grey") + ggtitle('Revenue Ratio Difference Distribution') + labs(x = "Revenue Ratio Difference", y = "Count")
```

```{r echo=FALSE}
confint(boot_eaby, level=0.95)
```

Conclusion: From the summary above, we can see the 95% confidence interval falls below 0, which means with 95% confident level, the real difference between the revenue ratio mean of treatment group and control group is in the range of around -9% and -1%. Thus, I conclude that paid search advertising does boost the revenue.

## Problem 3 Iron Bank

In the case, I set the null hypothesis as that the proportion of flagged trades from Iron Bank is the same as 2.4%. In other words, they were clean under this algorithm.

My test statistic will be the proportion of flagged trades out of 2021 in each simulation.

Let us get into the simulation part. First, the histogram below shows distribution of the proportion in 100000 simulations:

```{r echo=FALSE}
simulations = 100000
prob = 0.024
trades = 2021
results = data.frame(result = rbinom(simulations,trades,prob))
results = results %>% mutate(proportion = result/trades)
```

```{r echo=FALSE}
results %>% ggplot(aes(x=proportion)) + geom_histogram(bins=30,color="black", fill="brown") + ggtitle('Flagged Trades Proportion Distribution') + labs(x = "Flagged Trades Proportion", y = "Count")
```

Next, let us look at the P-value:

```{r echo=FALSE}
real_proportion = 70/trades
p_value = sum(results$proportion >= real_proportion)/ 100000
show(p_value)
```

As we see, the P value is around 0.2%, which means given that the null hypothesis is true, the chance of observing the number of flagged trades to be equal to or larger than 70 should be around 0.2%. It is very close to 0, so the null hypothesis fails to hold so that we claim that Iron Bank violated the "Inside Trading" laws.

One_sentence conclusion: If the P value is larger than 2.5%, then the null hypothesis will look plausible to me. 

Defensive statements: My presumptive confidence level is 95% and since this is one-side test, we should ignore the probability on the lower extreme side. Therefore, 95% confidence interval + 2.5%(the chance of the other extreme side) = 97.5%. Thus, if the P value here is larger than 2.5%, then it would be in 95% confidence interval. Hence, the null hypothesis fails to be rejected. Therefore, I would claim that there is not a significant evidence showing that the proportion of flagged trades from Iron Bank is not 2.4%.


## Problem 4 Milk, Demend, Revisited

```{r echo=FALSE}
milk = read.csv('/Users/kevin/Academic/MAE/2023 Summer/R&Prob&Stat/Assignments/Data/milk.csv',header = TRUE)
```

```{r echo=FALSE}
boot_milk = do(10000)*lm(log(sales) ~ log(price), data=resample(milk)) 
boot_milk = boot_milk %>% select(log.price.) 
```

```{r echo=FALSE, message=FALSE}
conf = confint(boot_milk$log.price., level=0.95)
annotations <- data.frame(
  x = c(conf$`2.5%`%>%round(2),conf$`97.5%`%>%round(2)),
  y = c(1500,1500),
  label = c("Lower Bound:", "Upper Bound:")
) 

```

```{r echo=FALSE}
ggplot(boot_milk,aes(x=log.price.)) + geom_histogram(bins=20,color="black", fill="purple") + ggtitle('Price Elasticity Distribution') + labs(x = "Price Elasticity ", y = "Count")  + geom_text(data = annotations,aes(x = x, y = y, label = paste(label, x)), size = 3, fontface = "bold")
```

Caption: This figure shows 10000 times bootstrap of linear regression of demand on the price. X-axis represents the price elasticity of demand and the y-axis stands for count for the values of the price elasticity of demand. In this graph, 'lower Bound' means the lower bound of 95% confidence interval and 'Upper Bound' means the upper bound of 95% confidence interval. Thus, the whole 95% confidence interval is from 'lower Bound' to 'Upper Bound'.

Conclusion: From the plot above, we can simply conclude that the idealistic price elasticity of demand should be between "Lower Bound" and "Upper Bound". i.e. As price goes up 1%, the demand would go down between |'lower Bound'%| and |'Upper Bound'%|.

## Problem 5 Standard-error Calculations

### Part A

#### i

First, we have

$$
E(\hat{p}-\hat{q}) = E(\hat{p})-E(\hat{q})
$$

Then, we have

$$
E(\hat{p}) = E(\overline{X_N}) = E(\frac{\sum_{i=1}^{N} X_i}{N}) = \frac{Np}{N} = p
$$

Also, we have

$$
E(\hat{q}) = E(\overline{Y_M}) = E(\frac{\sum_{i=1}^{M} Y_i}{M}) = \frac{Mq}{M} = q
$$

Thus, we have

$$
E(\hat{p}-\hat{q}) = E(\hat{p})-E(\hat{q})=p-q
$$

#### ii

As we know, the variance of a random variable X following Bernoulli distribution should be:

$$
Var(X) = p(1-p)
$$


$$
Var(\hat{p}) = Var(\overline{X_N}) = Var(\frac{X_1+X_2+...+X_N}{N})\\=\frac{1}{N^2}\sum_{i=1}^{N}Var(X_i) = \frac{1}{N^2}*Np(1-p)=\frac{p(1-p)}{N}
$$

Thus, the standard deviation should be

$$
Std(\hat{p}) = \sqrt{\frac{p(1-p)}{N}}
$$

#### iii

Following the same logic, we have

$$
Var(\hat{q}) =\frac{q(1-q)}{M}
$$

Thus, we have

$$
Var(\hat{\triangle})= Var((\hat{p}-\hat{q})=Var(\hat{p})-Var(\hat{q}) = \frac{p(1-p)}{N}-\frac{q(1-q)}{M}
$$

Therefore, we have

$$
Std(\hat{\triangle}) = \sqrt{\frac{p(1-p)}{N}-\frac{q(1-q)}{M}}
$$

### Part B

The expected value of this estimator should be

$$
E(\hat{\triangle}) = E(\overline{X_N}-\overline{Y_M})=E(\overline{X_N})-E(\overline{Y_M})=\mu_X-\mu_Y
$$

The standard error of this estimator should be

$$
Std(\hat{\triangle}) = \sqrt{Var(\hat{\triangle})}=\sqrt{Var(\overline{X_N}-\overline{Y_M})}=\sqrt{Var(\overline{X_N})-Var(\overline{Y_M})}\\=\sqrt{\frac{1}{N^2}\sum_{i=1}^{N}Var(X_i)+\frac{1}{M^2}\sum_{i=1}^{M}Var(Y_i)}=\sqrt{\frac{N}{N^2}{\sigma_X}^2+\frac{M}{M^2}{\sigma_Y}^2}=\sqrt{\frac{{\sigma_X}^2}{N}-\frac{{\sigma_Y}^2}{M}}
$$








